{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AggPze6nLRi1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def execution_timer(func):\n",
        "  @wraps(func)\n",
        "  def wrapper(*args, **kwargs):\n",
        "    start_time = time.perf_counter()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.perf_counter()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Function {func.__name__} executed in {elapsed_time:.4f} seconds\")\n",
        "    return result\n",
        "  return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lzvA7c7sSzJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Splits the image into patches and embeds them.\"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_chans=1, embed_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Convolution to handle patch extraction and linear projection (Embedding)\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x shape: [B, C, H, W] (e.g., [64, 1, 28, 28] for MNIST)\n",
        "\n",
        "        # 1. Patch Extraction + Projection (B, E, H/P, W/P)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # 2. Flatten patches (B, E, N_patches)\n",
        "        x = x.flatten(2)\n",
        "\n",
        "        # 3. Transpose to sequence format (B, N_patches, E)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1jARmE-WTKzs"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        # Multi-Head Attention (Self-Attention)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Multi-Layer Perceptron (Feed Forward Network)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # MHA with Residual Connection\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_output, _ = self.attn(x_norm, x_norm, x_norm)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # MLP with Residual Connection\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oJvDWcOBTWU8"
      },
      "outputs": [],
      "source": [
        "class ViTMini(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=28,\n",
        "                 patch_size=7, # 28 / 7 = 4x4 patches = 16 patches\n",
        "                 in_chans=1,\n",
        "                 num_classes=10,\n",
        "                 embed_dim=64,\n",
        "                 depth=2,\n",
        "                 num_heads=4,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        # 1. Patch Embedding\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # 2. [CLS] token and Positional Embeddings\n",
        "        # CLS Token: Used to pool the final sequence representation\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # Positional Embeddings: Size is num_patches + 1 for the CLS token\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        # 3. Transformer Encoder Blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # 4. Final Classification Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, C, H, W]\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # 1. Patch Embeddings -> x shape: [B, N_patches, E]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # 2. Prepend CLS token and add Positional Embeddings\n",
        "        # Expand CLS token to match batch size: [B, 1, E]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        # Concatenate CLS token to the front: [B, N_patches + 1, E]\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add Positional Embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # 3. Pass through Transformer Blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # 4. Classification\n",
        "        # We only take the output corresponding to the CLS token (index 0)\n",
        "        x = self.norm(x)\n",
        "        cls_output = x[:, 0]\n",
        "        x = self.head(cls_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTkJog6Top6",
        "outputId": "ccc9dde4-980e-475f-baa7-93a34089eead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 37.3MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.06MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 9.66MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.07MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting ViT Training ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:04<00:00,  7.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train executed in 64.1436 seconds\n",
            "Epoch 1 Loss: 0.7378\n",
            "\n",
            "Test set: Accuracy: 9174/10000 (91.74%)\n",
            "Function test executed in 3.7225 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:10<00:00,  6.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train executed in 70.0888 seconds\n",
            "Epoch 2 Loss: 0.2788\n",
            "\n",
            "Test set: Accuracy: 9512/10000 (95.12%)\n",
            "Function test executed in 3.8052 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:00<00:00,  7.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train executed in 60.8065 seconds\n",
            "Epoch 3 Loss: 0.2015\n",
            "\n",
            "Test set: Accuracy: 9595/10000 (95.95%)\n",
            "Function test executed in 3.6441 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:00<00:00,  7.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train executed in 60.4224 seconds\n",
            "Epoch 4 Loss: 0.1658\n",
            "\n",
            "Test set: Accuracy: 9651/10000 (96.51%)\n",
            "Function test executed in 3.5391 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:00<00:00,  7.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train executed in 60.4480 seconds\n",
            "Epoch 5 Loss: 0.1436\n",
            "\n",
            "Test set: Accuracy: 9705/10000 (97.05%)\n",
            "Function test executed in 3.6426 seconds\n"
          ]
        }
      ],
      "source": [
        "# --- Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# MNIST Images are 28x28. We choose a patch size that divides 28 evenly.\n",
        "PATCH_SIZE = 7 # 28 / 7 = 4 -> 16 total patches\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load Data\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
        "\n",
        "# Initialize Model\n",
        "model = ViTMini(\n",
        "    img_size=28,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    in_chans=1,\n",
        "    num_classes=10,\n",
        "    embed_dim=64, # Small embedding size for fast training\n",
        "    depth=2,      # Shallow transformer\n",
        "    num_heads=4   # 4 attention heads\n",
        ").to(device)\n",
        "\n",
        "# --- Training Loop ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "\n",
        "@execution_timer\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in tqdm(loader, desc=\"Training\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        log_probs = F.log_softmax(output, dim=1)\n",
        "        correct_log_probs = log_probs.gather(dim=1, index=target.view(-1, 1)).squeeze()\n",
        "        loss = -correct_log_probs.mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@execution_timer\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    accuracy = 100. * correct / len(loader.dataset)\n",
        "    print(f'\\nTest set: Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
        "    return accuracy\n",
        "\n",
        "print(\"\\n--- Starting ViT Training ---\")\n",
        "for epoch in range(epochs):\n",
        "    loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch {epoch+1} Loss: {loss:.4f}\")\n",
        "    test(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mnQHT7uVUQ_M"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, symbol=None, frequency=None):\n",
        "    self.symbol = symbol\n",
        "    self.frequency = frequency\n",
        "    self.left = None\n",
        "    self.right = None\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.frequency < other.frequency\n",
        "\n",
        "  def is_leaf(self):\n",
        "    return self.left is None and self.right is None\n",
        "\n",
        "def build_tree(leaf_nodes, frequencies):\n",
        "  # Create a priority queue of nodes\n",
        "  priority_queue = [Node(val, freq) for val, freq in zip(leaf_nodes, frequencies)]\n",
        "  heapq.heapify(priority_queue)\n",
        "\n",
        "  internal_node_counter = 0\n",
        "  # Build the Huffman tree\n",
        "  while len(priority_queue) > 1:\n",
        "    left_child = heapq.heappop(priority_queue)\n",
        "    right_child = heapq.heappop(priority_queue)\n",
        "    merged_node = Node(\n",
        "      symbol=f'Internal Node {internal_node_counter}', frequency=left_child.frequency + right_child.frequency\n",
        "    )\n",
        "    merged_node.left = left_child\n",
        "    merged_node.right = right_child\n",
        "    heapq.heappush(priority_queue, merged_node)\n",
        "  return priority_queue[0]\n",
        "\n",
        "def generate_paths(node, code, path_dict):\n",
        "  if node is not None:\n",
        "    if node.symbol is not None and not isinstance(node.symbol, str):\n",
        "      path_dict[node.symbol] = code\n",
        "    generate_paths(node.left, code + [0], path_dict)\n",
        "    generate_paths(node.right, code + [1], path_dict)\n",
        "  return path_dict\n",
        "\n",
        "def max_depth(node):\n",
        "  if node is None:\n",
        "    return 0\n",
        "  left_depth = max_depth(node.left)\n",
        "  right_depth = max_depth(node.right)\n",
        "  return max(left_depth, right_depth) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maERiPzmVD3F",
        "outputId": "20c3bcf0-8ac5-4143-abb6-08af502cd4bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paths for classes: {4: [0, 0, 0], 8: [0, 0, 1], 6: [0, 1, 0], 2: [0, 1, 1], 3: [1, 0, 0], 5: [1, 0, 1], 0: [1, 1, 0, 0], 1: [1, 1, 0, 1], 7: [1, 1, 1, 0], 9: [1, 1, 1, 1]}\n",
            "All items in vocabulary present in tree: True\n",
            "Number of items in vocabulary not present in tree: 0\n"
          ]
        }
      ],
      "source": [
        "classes = list(range(10))\n",
        "root_node = build_tree(classes, [1] * 10)\n",
        "paths = generate_paths(root_node, [], {})\n",
        "print(f\"Paths for classes: {paths}\")\n",
        "print(f\"All items in vocabulary present in tree: {all(item in paths for item in classes)}\")\n",
        "print(f\"Number of items in vocabulary not present in tree: {len([item for item in classes if item not in paths])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H6oEOOOgV7LM"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class HierarchicalSoftmaxNodeViT(nn.Module):\n",
        "    def __init__(self, root, hidden_size):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.hidden_size = hidden_size\n",
        "        print(f\"Using hidden size: {self.hidden_size}\")\n",
        "        self.paths = generate_paths(root, [], {})\n",
        "        self.node_name_map = {}\n",
        "        self.node_weights = nn.ModuleDict()\n",
        "        self.param_counter = 0\n",
        "\n",
        "        def initialize_node_parameters(node):\n",
        "          if node is None or node.is_leaf():\n",
        "            return None\n",
        "          node_str = str(self.param_counter)\n",
        "          self.node_name_map[node] = node_str\n",
        "          self.node_weights[node_str] = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "          self.param_counter += 1\n",
        "          initialize_node_parameters(node.left)\n",
        "          initialize_node_parameters(node.right)\n",
        "\n",
        "        initialize_node_parameters(self.root)\n",
        "        print(f\"HSM initialized with {len(self.node_weights)} internal nodes\")\n",
        "\n",
        "\n",
        "    # NOTE: The forward pass for the loss calculation is slightly simplified\n",
        "    # for the ViT classification head where the input is [B, E] and the target is [B]\n",
        "    def forward(self, hidden_state, target_ids):\n",
        "        total_loss = torch.tensor(0.0)\n",
        "        total_loss.requires_grad = True\n",
        "        total_valid_tokens = 0\n",
        "\n",
        "        for h_i, target_id in zip(hidden_state, target_ids):\n",
        "          path_step_loss = []\n",
        "          target = target_id.item()\n",
        "          if target not in self.paths:\n",
        "            continue\n",
        "          choices = self.paths[target]\n",
        "          curr = self.root\n",
        "          for choice in choices:\n",
        "            if curr.is_leaf():\n",
        "              break\n",
        "            node_str = self.node_name_map[curr]\n",
        "            W = self.node_weights[node_str]\n",
        "            binary_loss = F.binary_cross_entropy_with_logits(\n",
        "                W(h_i),\n",
        "                torch.tensor([float(choice)], device=device),\n",
        "                reduction='sum'\n",
        "            )\n",
        "            path_step_loss.append(binary_loss)\n",
        "            curr = curr.left if not choice else curr.right\n",
        "          total_loss = total_loss + torch.stack(path_step_loss).sum()\n",
        "          total_valid_tokens += 1\n",
        "        return total_loss / max(1, total_valid_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rGyk83sGYPiZ"
      },
      "outputs": [],
      "source": [
        "class ViTMiniHSM(ViTMini):\n",
        "    def __init__(self, **kwargs):\n",
        "        # Call base constructor to set up patch embedding and transformer blocks\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # 1. Build the HSM Tree (10 classes: 0-9)\n",
        "        self.root = build_tree(list(range(10)), [1] * 10)\n",
        "        print(f\"Paths for classes: {paths}\")\n",
        "        print(f\"All items in vocabulary present in tree: {all(item in paths for item in classes)}\")\n",
        "        print(f\"Number of items in vocabulary not present in tree: {len([item for item in classes if item not in paths])}\")\n",
        "\n",
        "        # 2. Replace the standard head with the HSM head\n",
        "        self.hsm_head = HierarchicalSoftmaxNodeViT(\n",
        "            root=self.root,\n",
        "            hidden_size=kwargs['embed_dim'] # Use the CLS token's embedding dimension\n",
        "        )\n",
        "\n",
        "        # Remove the standard linear head defined in the base class\n",
        "        del self.head\n",
        "\n",
        "    def _greedy_predict(self, hidden_state):\n",
        "        curr = self.hsm_head.root\n",
        "        while not curr.is_leaf():\n",
        "          node_str = self.hsm_head.node_name_map[curr]\n",
        "          W = self.hsm_head.node_weights[node_str]\n",
        "          choice = F.sigmoid(W(hidden_state)) > 0.5\n",
        "          curr = curr.left if not choice else curr.right\n",
        "        return curr.symbol\n",
        "\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # 1. Base Transformer Forward Pass (same as original ViT)\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        cls_output = self.norm(x)[:, 0]\n",
        "\n",
        "        # 3. Head Calculation\n",
        "        if targets is not None:\n",
        "            # Targets are the true class labels [B]\n",
        "            loss = self.hsm_head(cls_output, targets)\n",
        "            return {\"loss\": loss}\n",
        "        else:\n",
        "            predictions = []\n",
        "            for i in range(B):\n",
        "                h_i = cls_output[i]\n",
        "                predicted_class_id = self._greedy_predict(h_i)\n",
        "                predictions.append(predicted_class_id)\n",
        "            predicted_classes = torch.tensor(predictions, dtype=torch.long, device=cls_output.device)\n",
        "            return predicted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WUQVJCWcZnAS"
      },
      "outputs": [],
      "source": [
        "# --- Updated Training and Testing Logic ---\n",
        "\n",
        "@execution_timer\n",
        "def train_hsm_vit(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, target in tqdm(loader, desc=\"Training (HSM)\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model returns a dictionary {\"loss\": loss_tensor}\n",
        "        output_dict = model(data, targets=target)\n",
        "        loss = output_dict['loss']\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@execution_timer\n",
        "def test_hsm_vit(model, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the ViT-HSM model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(loader, desc=\"Testing\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # New Step: Call model without targets to trigger the prediction block\n",
        "            # predicted_classes shape: [B] (Tensor of integer class IDs)\n",
        "            predicted_classes = model(data)\n",
        "\n",
        "            # Compare the predicted tensor to the target tensor\n",
        "            correct += predicted_classes.eq(target).sum().item()\n",
        "            total_samples += target.size(0)\n",
        "\n",
        "    accuracy = 100. * correct / total_samples\n",
        "    print(f'\\nðŸ“ˆ HSM Test set: Accuracy: {correct}/{total_samples} ({accuracy:.2f}%)')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mWXfaftZvbn",
        "outputId": "09a82ef1-ae76-4e01-f737-8fdeadb0c1cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paths for classes: {4: [0, 0, 0], 8: [0, 0, 1], 6: [0, 1, 0], 2: [0, 1, 1], 3: [1, 0, 0], 5: [1, 0, 1], 0: [1, 1, 0, 0], 1: [1, 1, 0, 1], 7: [1, 1, 1, 0], 9: [1, 1, 1, 1]}\n",
            "All items in vocabulary present in tree: True\n",
            "Number of items in vocabulary not present in tree: 0\n",
            "Using hidden size: 64\n",
            "HSM initialized with 9 internal nodes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (HSM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:37<00:00,  4.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train_hsm_vit executed in 97.0251 seconds\n",
            "Epoch 1 HSM Loss: 0.8429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:04<00:00, 16.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ HSM Test set: Accuracy: 9112/10000 (91.12%)\n",
            "Function test_hsm_vit executed in 4.9252 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (HSM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:37<00:00,  4.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train_hsm_vit executed in 97.9310 seconds\n",
            "Epoch 2 HSM Loss: 0.3382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:04<00:00, 15.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ HSM Test set: Accuracy: 9413/10000 (94.13%)\n",
            "Function test_hsm_vit executed in 4.9489 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (HSM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:36<00:00,  4.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train_hsm_vit executed in 96.4398 seconds\n",
            "Epoch 3 HSM Loss: 0.2407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:04<00:00, 16.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ HSM Test set: Accuracy: 9515/10000 (95.15%)\n",
            "Function test_hsm_vit executed in 4.8450 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (HSM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:35<00:00,  4.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train_hsm_vit executed in 95.1698 seconds\n",
            "Epoch 4 HSM Loss: 0.1943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:04<00:00, 16.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ HSM Test set: Accuracy: 9619/10000 (96.19%)\n",
            "Function test_hsm_vit executed in 4.7781 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (HSM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [01:35<00:00,  4.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function train_hsm_vit executed in 95.1129 seconds\n",
            "Epoch 5 HSM Loss: 0.1631\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:04<00:00, 16.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“ˆ HSM Test set: Accuracy: 9654/10000 (96.54%)\n",
            "Function test_hsm_vit executed in 4.8181 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize Model\n",
        "model_hsm = ViTMiniHSM(\n",
        "    img_size=28, patch_size=7, in_chans=1, num_classes=10,\n",
        "    embed_dim=64, depth=2, num_heads=4\n",
        ").to(device)\n",
        "\n",
        "# --- Execute Training ---\n",
        "epochs = 5\n",
        "optimizer_hsm = optim.Adam(model_hsm.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = train_hsm_vit(model_hsm, train_loader, optimizer_hsm, device)\n",
        "    print(f\"Epoch {epoch+1} HSM Loss: {loss:.4f}\")\n",
        "    test_hsm_vit(model_hsm, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
